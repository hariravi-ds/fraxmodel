{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01553d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, HistGradientBoostingClassifier\n",
    "\n",
    "import joblib\n",
    "\n",
    "\n",
    "# -------------------- helpers --------------------\n",
    "def summarize(name: str, arr):\n",
    "    arr = np.asarray(arr, dtype=float)\n",
    "    print(f\"\\n{name} summary:\")\n",
    "    print(\"  min:\", float(np.min(arr)))\n",
    "    print(\"  p50:\", float(np.percentile(arr, 50)))\n",
    "    print(\"  p90:\", float(np.percentile(arr, 90)))\n",
    "    print(\"  p99:\", float(np.percentile(arr, 99)))\n",
    "    print(\"  max:\", float(np.max(arr)))\n",
    "\n",
    "\n",
    "def tail_metrics(name: str, y_true: np.ndarray, y_pred: np.ndarray, thresholds=(3.0, 5.0, 10.0)):\n",
    "    print(f\"\\n{name} tail metrics:\")\n",
    "    for t in thresholds:\n",
    "        mask = y_true >= t\n",
    "        n = int(mask.sum())\n",
    "        if n < 5:\n",
    "            print(f\"  y >= {t}: n={n} (too few)\")\n",
    "            continue\n",
    "        mae = mean_absolute_error(y_true[mask], y_pred[mask])\n",
    "        r2 = r2_score(y_true[mask], y_pred[mask])\n",
    "        print(f\"  y >= {t}: n={n} | MAE={mae:.3f} | R2={r2:.4f}\")\n",
    "\n",
    "\n",
    "def _prep_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = df.copy()\n",
    "    # Drop known non-feature cols if present\n",
    "    X = X.drop(\n",
    "        columns=[c for c in [\"continent\", \"bmi_units\", \"scanner\",\n",
    "                             \"mof_risk\", \"hip_risk\"] if c in X.columns],\n",
    "        errors=\"ignore\"\n",
    "    )\n",
    "\n",
    "    # Normalize and one-hot encode us_group\n",
    "    if \"us_group\" in X.columns:\n",
    "        X[\"us_group\"] = X[\"us_group\"].astype(str).str.strip()\n",
    "        # remove any pre-dummified columns to avoid duplicates\n",
    "        X = X.drop(columns=[c for c in X.columns if c.startswith(\n",
    "            \"us_group_\")], errors=\"ignore\")\n",
    "        X = pd.get_dummies(X, columns=[\"us_group\"], drop_first=False)\n",
    "\n",
    "    # Coerce numeric (Excel/CSV might have strings)\n",
    "    X = X.apply(pd.to_numeric, errors=\"coerce\")\n",
    "    return X\n",
    "\n",
    "\n",
    "# -------------------- TRAIN + SAVE --------------------\n",
    "def train_and_save_fraxplus_models(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    model_path: str = \"fraxplus_models.pkl\",\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    hip_high_threshold: float = 2.0,\n",
    "    eval_clamp: bool = False,\n",
    "    prod_clamp_max: float = 100.0,\n",
    "    tail_thresholds=(3.0, 5.0, 10.0),\n",
    "):\n",
    "\n",
    "    # ---- Build aligned dataset ONCE (prevents label/feature mismatch) ----\n",
    "    base = df.drop(columns=[c for c in [\n",
    "                   \"continent\", \"bmi_units\", \"scanner\"] if c in df.columns], errors=\"ignore\").copy()\n",
    "    base = base.dropna(subset=[\"mof_risk\", \"hip_risk\"]).copy()\n",
    "    if \"us_group\" in base.columns:\n",
    "        base = base.dropna(subset=[\"us_group\"]).copy()\n",
    "\n",
    "    y_mof = base[\"mof_risk\"].astype(float).to_numpy()\n",
    "    y_hip = base[\"hip_risk\"].astype(float).to_numpy()\n",
    "\n",
    "    X = _prep_features(base)  # one-hot us_group + numeric coercion\n",
    "\n",
    "    # ---- Split ----\n",
    "    X_train, X_test, y_mof_train, y_mof_test, y_hip_train, y_hip_test = train_test_split(\n",
    "        X, y_mof, y_hip, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # ---- MOF model ----\n",
    "    mof_model = HistGradientBoostingRegressor(\n",
    "        learning_rate=0.05,\n",
    "        max_iter=2000,\n",
    "        l2_regularization=0.5,\n",
    "        early_stopping=True,\n",
    "        random_state=random_state,\n",
    "        max_depth=4,\n",
    "        min_samples_leaf=20,\n",
    "    )\n",
    "    print(\"\\nTraining MOF (log1p)... {'max_depth': 4, 'min_samples_leaf': 20}\")\n",
    "    mof_model.fit(X_train, np.log1p(y_mof_train))\n",
    "\n",
    "    mof_pred_t = mof_model.predict(X_test)\n",
    "    mof_pred = np.expm1(mof_pred_t)\n",
    "\n",
    "    # ---- HIP two-stage models ----\n",
    "    print(f\"Training Hip classifier (hip_risk >= {hip_high_threshold})...\")\n",
    "    y_high = (y_hip_train >= hip_high_threshold).astype(int)\n",
    "\n",
    "    hip_clf = HistGradientBoostingClassifier(\n",
    "        learning_rate=0.05,\n",
    "        max_iter=2000,\n",
    "        l2_regularization=0.5,\n",
    "        early_stopping=True,\n",
    "        random_state=random_state,\n",
    "        max_depth=3,\n",
    "        min_samples_leaf=40,\n",
    "    )\n",
    "    hip_clf.fit(X_train, y_high)\n",
    "\n",
    "    low_mask = y_hip_train < hip_high_threshold\n",
    "    high_mask = ~low_mask\n",
    "    print(f\"Training Hip regressor LOW (n={int(low_mask.sum())})...\")\n",
    "    print(f\"Training Hip regressor HIGH (n={int(high_mask.sum())})...\")\n",
    "\n",
    "    hip_reg_low = HistGradientBoostingRegressor(\n",
    "        learning_rate=0.05,\n",
    "        max_iter=3000,\n",
    "        l2_regularization=0.5,\n",
    "        early_stopping=True,\n",
    "        random_state=random_state,\n",
    "        max_depth=3,\n",
    "        min_samples_leaf=40,\n",
    "    )\n",
    "    hip_reg_high = HistGradientBoostingRegressor(\n",
    "        learning_rate=0.03,\n",
    "        max_iter=4000,\n",
    "        l2_regularization=0.2,\n",
    "        early_stopping=True,\n",
    "        random_state=random_state,\n",
    "        max_depth=4,\n",
    "        min_samples_leaf=15,\n",
    "    )\n",
    "\n",
    "    hip_reg_low.fit(X_train[low_mask], np.log1p(y_hip_train[low_mask]))\n",
    "    hip_reg_high.fit(X_train[high_mask], np.log1p(y_hip_train[high_mask]))\n",
    "\n",
    "    # ---- HIP prediction in log space ----\n",
    "    p_high = hip_clf.predict_proba(X_test)[:, 1]\n",
    "    hip_low_t = hip_reg_low.predict(X_test)\n",
    "    hip_high_t = hip_reg_high.predict(X_test)\n",
    "    hip_pred_t = (1.0 - p_high) * hip_low_t + \\\n",
    "        p_high * hip_high_t  # log1p space\n",
    "\n",
    "    # ---- HIP calibration in LOG space ----\n",
    "    p_high_tr = hip_clf.predict_proba(X_train)[:, 1]\n",
    "    low_tr_t = hip_reg_low.predict(X_train)\n",
    "    high_tr_t = hip_reg_high.predict(X_train)\n",
    "    hip_pred_tr_t = (1.0 - p_high_tr) * low_tr_t + \\\n",
    "        p_high_tr * high_tr_t\n",
    "\n",
    "    y_hip_train_t = np.log1p(y_hip_train)\n",
    "    Xcal = np.vstack([np.ones_like(hip_pred_tr_t), hip_pred_tr_t]).T\n",
    "    a, b = np.linalg.lstsq(Xcal, y_hip_train_t, rcond=None)[0]\n",
    "    a, b = float(a), float(b)\n",
    "    print(f\"\\nHip calib (LOG space) a={a:.6f}, b={b:.6f}\")\n",
    "\n",
    "    # Apply calibration + CLIP in log space to avoid blow-ups\n",
    "    hip_pred_t_cal = a + b * hip_pred_t\n",
    "    hip_pred_t_cal = np.clip(hip_pred_t_cal, 0.0, np.log1p(prod_clamp_max))\n",
    "    hip_pred = np.expm1(hip_pred_t_cal)\n",
    "\n",
    "    # Optional: clamp MOF for evaluation\n",
    "    if eval_clamp:\n",
    "        mof_pred = np.clip(mof_pred, 0.0, prod_clamp_max)\n",
    "        hip_pred = np.clip(hip_pred, 0.0, prod_clamp_max)\n",
    "\n",
    "    # ---- Print metrics ----\n",
    "    print(\"\\n=== PERFORMANCE (holdout) ===\")\n",
    "    print(\n",
    "        f\"MOF  original: MAE={mean_absolute_error(y_mof_test, mof_pred):.3f} | \"\n",
    "        f\"R2={r2_score(y_mof_test, mof_pred):.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"MOF  log1p:    MAE={mean_absolute_error(np.log1p(y_mof_test), mof_pred_t):.3f} | \"\n",
    "        f\"R2={r2_score(np.log1p(y_mof_test), mof_pred_t):.4f}\"\n",
    "    )\n",
    "\n",
    "    # Hip metrics\n",
    "    hip_true_t = np.log1p(y_hip_test)\n",
    "    print(\n",
    "        f\"Hip  original (cal): MAE={mean_absolute_error(y_hip_test, hip_pred):.3f} | \"\n",
    "        f\"R2={r2_score(y_hip_test, hip_pred):.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Hip  log1p    (cal): MAE={mean_absolute_error(hip_true_t, hip_pred_t_cal):.3f} | \"\n",
    "        f\"R2={r2_score(hip_true_t, hip_pred_t_cal):.4f}\"\n",
    "    )\n",
    "\n",
    "    tail_metrics(\"MOF\", y_mof_test, mof_pred, thresholds=tail_thresholds)\n",
    "    tail_metrics(\"Hip (cal)\", y_hip_test, hip_pred, thresholds=tail_thresholds)\n",
    "\n",
    "    # Debug summaries\n",
    "    summarize(\"y_hip_test\", y_hip_test)\n",
    "    summarize(\"hip_pred\", hip_pred)\n",
    "\n",
    "    # ---- Save bundle ----\n",
    "    bundle = {\n",
    "        \"mof_model\": mof_model,\n",
    "        \"hip_clf\": hip_clf,\n",
    "        \"hip_reg_low\": hip_reg_low,\n",
    "        \"hip_reg_high\": hip_reg_high,\n",
    "        \"feature_columns\": list(X.columns),\n",
    "        \"hip_high_threshold\": float(hip_high_threshold),\n",
    "        \"hip_calib_a\": a,\n",
    "        \"hip_calib_b\": b,\n",
    "        \"hip_calib_log_clip_max\": float(prod_clamp_max),  # 100 by default\n",
    "        \"schema_version\": 2,\n",
    "    }\n",
    "    joblib.dump(bundle, model_path)\n",
    "    print(f\"\\nModels saved to {model_path}\")\n",
    "    return bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f69d259",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('fraxdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1011dd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training MOF (log1p)... {'max_depth': 4, 'min_samples_leaf': 20}\n",
      "Training Hip classifier (hip_risk >= 2.0)...\n",
      "Training Hip regressor LOW (n=215)...\n",
      "Training Hip regressor HIGH (n=185)...\n",
      "\n",
      "Hip calib (LOG space) a=-0.014073, b=1.016230\n",
      "\n",
      "=== PERFORMANCE (holdout) ===\n",
      "MOF  original: MAE=0.799 | R2=0.9569\n",
      "MOF  log1p:    MAE=0.093 | R2=0.9581\n",
      "Hip  original (cal): MAE=0.414 | R2=0.9730\n",
      "Hip  log1p    (cal): MAE=0.123 | R2=0.9560\n",
      "\n",
      "MOF tail metrics:\n",
      "  y >= 3.0: n=76 | MAE=0.915 | R2=0.9459\n",
      "  y >= 5.0: n=58 | MAE=1.122 | R2=0.9259\n",
      "  y >= 10.0: n=29 | MAE=1.474 | R2=0.8630\n",
      "\n",
      "Hip (cal) tail metrics:\n",
      "  y >= 3.0: n=34 | MAE=0.658 | R2=0.9596\n",
      "  y >= 5.0: n=18 | MAE=0.854 | R2=0.9506\n",
      "  y >= 10.0: n=4 (too few)\n",
      "\n",
      "y_hip_test summary:\n",
      "  min: 0.0\n",
      "  p50: 1.6\n",
      "  p90: 6.1\n",
      "  p99: 18.0\n",
      "  max: 18.0\n",
      "\n",
      "hip_pred summary:\n",
      "  min: 0.0\n",
      "  p50: 1.257778520564137\n",
      "  p90: 7.077303455642447\n",
      "  p99: 18.287632325419853\n",
      "  max: 19.914419458722204\n",
      "\n",
      "Models saved to fraxplus_models.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mof_model': HistGradientBoostingRegressor(early_stopping=True, l2_regularization=0.5,\n",
       "                               learning_rate=0.05, max_depth=4, max_iter=2000,\n",
       "                               random_state=42),\n",
       " 'hip_clf': HistGradientBoostingClassifier(early_stopping=True, l2_regularization=0.5,\n",
       "                                learning_rate=0.05, max_depth=3, max_iter=2000,\n",
       "                                min_samples_leaf=40, random_state=42),\n",
       " 'hip_reg_low': HistGradientBoostingRegressor(early_stopping=True, l2_regularization=0.5,\n",
       "                               learning_rate=0.05, max_depth=3, max_iter=3000,\n",
       "                               min_samples_leaf=40, random_state=42),\n",
       " 'hip_reg_high': HistGradientBoostingRegressor(early_stopping=True, l2_regularization=0.2,\n",
       "                               learning_rate=0.03, max_depth=4, max_iter=4000,\n",
       "                               min_samples_leaf=15, random_state=42),\n",
       " 'feature_columns': ['age',\n",
       "  'sex_female',\n",
       "  'weight_kg',\n",
       "  'height_cm',\n",
       "  'bmi',\n",
       "  'bmd',\n",
       "  'previousFracture',\n",
       "  'parentFracturedHip',\n",
       "  'smoking',\n",
       "  'glucocorticoids',\n",
       "  'rheumatoidArthritis',\n",
       "  'secondaryOsteoporosis',\n",
       "  'alcohol',\n",
       "  't_score',\n",
       "  'us_group_US (Asian)',\n",
       "  'us_group_US (Black)',\n",
       "  'us_group_US (Caucasian)',\n",
       "  'us_group_US (Hispanic)'],\n",
       " 'hip_high_threshold': 2.0,\n",
       " 'hip_calib_a': -0.014072645935410666,\n",
       " 'hip_calib_b': 1.0162300570817109,\n",
       " 'hip_calib_log_clip_max': 100.0,\n",
       " 'schema_version': 2}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_save_fraxplus_models(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
